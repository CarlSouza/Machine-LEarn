{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import abc\n",
    "import hashlib\n",
    "import zipfile\n",
    "import glob\n",
    "import logging\n",
    "import tarfile\n",
    "from skimage.io import imread\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliana\\Documents\\FGV\\ML\\TrabalhoFinal\\join_notebook\\utils\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from utils.datasets import get_dataloaders, get_img_size, DATASETS\n",
    "from utils.helpers import (create_safe_directory, get_device, set_seed, get_n_param,\n",
    "                           get_config_section, update_namespace_, FormatterNoDuplicate)\n",
    "\n",
    "CONFIG_FILE = \"hyperparam.ini\"\n",
    "RES_DIR = \"results\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "def matrix_log_density_gaussian(x, mu, logvar):\n",
    "    \"\"\"Calculates log density of a Gaussian for all combination of bacth pairs of\n",
    "    `x` and `mu`. I.e. return tensor of shape `(batch_size, batch_size, dim)`\n",
    "    instead of (batch_size, dim) in the usual log density.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "        Value at which to compute the density. Shape: (batch_size, dim).\n",
    "\n",
    "    mu: torch.Tensor\n",
    "        Mean. Shape: (batch_size, dim).\n",
    "\n",
    "    logvar: torch.Tensor\n",
    "        Log variance. Shape: (batch_size, dim).\n",
    "\n",
    "    batch_size: int\n",
    "        number of training images in the batch\n",
    "    \"\"\"\n",
    "    batch_size, dim = x.shape\n",
    "    x = x.view(batch_size, 1, dim)\n",
    "    mu = mu.view(1, batch_size, dim)\n",
    "    logvar = logvar.view(1, batch_size, dim)\n",
    "    return log_density_gaussian(x, mu, logvar)\n",
    "\n",
    "\n",
    "def log_density_gaussian(x, mu, logvar):\n",
    "    \"\"\"Calculates log density of a Gaussian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor or np.ndarray or float\n",
    "        Value at which to compute the density.\n",
    "\n",
    "    mu: torch.Tensor or np.ndarray or float\n",
    "        Mean.\n",
    "\n",
    "    logvar: torch.Tensor or np.ndarray or float\n",
    "        Log variance.\n",
    "    \"\"\"\n",
    "    normalization = - 0.5 * (math.log(2 * math.pi) + logvar)\n",
    "    inv_var = torch.exp(-logvar)\n",
    "    log_density = normalization - 0.5 * ((x - mu)**2 * inv_var)\n",
    "    return log_density\n",
    "\n",
    "\n",
    "def log_importance_weight_matrix(batch_size, dataset_size):\n",
    "    \"\"\"\n",
    "    Calculates a log importance weight matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size: int\n",
    "        number of training images in the batch\n",
    "\n",
    "    dataset_size: int\n",
    "    number of training images in the dataset\n",
    "    \"\"\"\n",
    "    N = dataset_size\n",
    "    M = batch_size - 1\n",
    "    strat_weight = (N - M) / (N * M)\n",
    "    W = torch.Tensor(batch_size, batch_size).fill_(1 / M)\n",
    "    W.view(-1)[::M + 1] = 1 / N\n",
    "    W.view(-1)[1::M + 1] = strat_weight\n",
    "    W[M - 1, 0] = strat_weight\n",
    "    return W.log()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def get_activation_name(activation):\n",
    "    \"\"\"Given a string or a `torch.nn.modules.activation` return the name of the activation.\"\"\"\n",
    "    if isinstance(activation, str):\n",
    "        return activation\n",
    "\n",
    "    mapper = {nn.LeakyReLU: \"leaky_relu\", nn.ReLU: \"relu\", nn.Tanh: \"tanh\",\n",
    "              nn.Sigmoid: \"sigmoid\", nn.Softmax: \"sigmoid\"}\n",
    "    for k, v in mapper.items():\n",
    "        if isinstance(activation, k):\n",
    "            return k\n",
    "\n",
    "    raise ValueError(\"Unkown given activation type : {}\".format(activation))\n",
    "\n",
    "\n",
    "def get_gain(activation):\n",
    "    \"\"\"Given an object of `torch.nn.modules.activation` or an activation name\n",
    "    return the correct gain.\"\"\"\n",
    "    if activation is None:\n",
    "        return 1\n",
    "\n",
    "    activation_name = get_activation_name(activation)\n",
    "\n",
    "    param = None if activation_name != \"leaky_relu\" else activation.negative_slope\n",
    "    gain = nn.init.calculate_gain(activation_name, param)\n",
    "\n",
    "    return gain\n",
    "\n",
    "\n",
    "def linear_init(layer, activation=\"relu\"):\n",
    "    \"\"\"Initialize a linear layer.\n",
    "    Args:\n",
    "        layer (nn.Linear): parameters to initialize.\n",
    "        activation (`torch.nn.modules.activation` or str, optional) activation that\n",
    "            will be used on the `layer`.\n",
    "    \"\"\"\n",
    "    x = layer.weight\n",
    "\n",
    "    if activation is None:\n",
    "        return nn.init.xavier_uniform_(x)\n",
    "\n",
    "    activation_name = get_activation_name(activation)\n",
    "\n",
    "    if activation_name == \"leaky_relu\":\n",
    "        a = 0 if isinstance(activation, str) else activation.negative_slope\n",
    "        return nn.init.kaiming_uniform_(x, a=a, nonlinearity='leaky_relu')\n",
    "    elif activation_name == \"relu\":\n",
    "        return nn.init.kaiming_uniform_(x, nonlinearity='relu')\n",
    "    elif activation_name in [\"sigmoid\", \"tanh\"]:\n",
    "        return nn.init.xavier_uniform_(x, gain=get_gain(activation))\n",
    "\n",
    "\n",
    "def weights_init(module):\n",
    "    if isinstance(module, torch.nn.modules.conv._ConvNd):\n",
    "        # TO-DO: check litterature\n",
    "        linear_init(module)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        linear_init(module)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_FILENAME = \"model.pt\"\n",
    "META_FILENAME = \"specs.json\"\n",
    "\n",
    "\n",
    "def save_model(model, directory, metadata=None, filename=MODEL_FILENAME):\n",
    "    \"\"\"\n",
    "    Save a model and corresponding metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Model.\n",
    "\n",
    "    directory : str\n",
    "        Path to the directory where to save the data.\n",
    "\n",
    "    metadata : dict\n",
    "        Metadata to save.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.cpu()\n",
    "\n",
    "    if metadata is None:\n",
    "        # save the minimum required for loading\n",
    "        metadata = dict(img_size=model.img_size, latent_dim=model.latent_dim,\n",
    "                        model_type=model.model_type)\n",
    "\n",
    "    save_metadata(metadata, directory)\n",
    "\n",
    "    path_to_model = os.path.join(directory, filename)\n",
    "    torch.save(model.state_dict(), path_to_model)\n",
    "\n",
    "    model.to(device)  # restore device\n",
    "\n",
    "\n",
    "def load_metadata(directory, filename=META_FILENAME):\n",
    "    \"\"\"Load the metadata of a training directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string\n",
    "        Path to folder where model is saved. For example './experiments/mnist'.\n",
    "    \"\"\"\n",
    "    path_to_metadata = os.path.join(directory, filename)\n",
    "\n",
    "    with open(path_to_metadata) as metadata_file:\n",
    "        metadata = json.load(metadata_file)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_metadata(metadata, directory, filename=META_FILENAME, **kwargs):\n",
    "    \"\"\"Load the metadata of a training directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata:\n",
    "        Object to save\n",
    "\n",
    "    directory: string\n",
    "        Path to folder where to save model. For example './experiments/mnist'.\n",
    "\n",
    "    kwargs:\n",
    "        Additional arguments to `json.dump`\n",
    "    \"\"\"\n",
    "    path_to_metadata = os.path.join(directory, filename)\n",
    "\n",
    "    with open(path_to_metadata, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4, sort_keys=True, **kwargs)\n",
    "\n",
    "\n",
    "def load_model(directory, is_gpu=True, filename=MODEL_FILENAME):\n",
    "    \"\"\"Load a trained model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string\n",
    "        Path to folder where model is saved. For example './experiments/mnist'.\n",
    "\n",
    "    is_gpu : bool\n",
    "        Whether to load on GPU is available.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and is_gpu\n",
    "                          else \"cpu\")\n",
    "\n",
    "    path_to_model = os.path.join(directory, MODEL_FILENAME)\n",
    "\n",
    "    metadata = load_metadata(directory)\n",
    "    img_size = metadata[\"img_size\"]\n",
    "    latent_dim = metadata[\"latent_dim\"]\n",
    "    model_type = metadata[\"model_type\"]\n",
    "\n",
    "    path_to_model = os.path.join(directory, filename)\n",
    "    model = _get_model(model_type, img_size, latent_dim, device, path_to_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoints(directory, is_gpu=True):\n",
    "    \"\"\"Load all chechpointed models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : string\n",
    "        Path to folder where model is saved. For example './experiments/mnist'.\n",
    "\n",
    "    is_gpu : bool\n",
    "        Whether to load on GPU .\n",
    "    \"\"\"\n",
    "    checkpoints = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            results = re.search(r'.*?-([0-9].*?).pt', filename)\n",
    "            if results is not None:\n",
    "                epoch_idx = int(results.group(1))\n",
    "                model = load_model(root, is_gpu=is_gpu, filename=filename)\n",
    "                checkpoints.append((epoch_idx, model))\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "\n",
    "def _get_model(model_type, img_size, latent_dim, device, path_to_model):\n",
    "    \"\"\" Load a single model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_type : str\n",
    "        The name of the model to load. For example Burgess.\n",
    "    img_size : tuple\n",
    "        Tuple of the number of pixels in the image width and height.\n",
    "        For example (32, 32) or (64, 64).\n",
    "    latent_dim : int\n",
    "        The number of latent dimensions in the bottleneck.\n",
    "\n",
    "    device : str\n",
    "        Either 'cuda' or 'cpu'\n",
    "    path_to_device : str\n",
    "        Full path to the saved model on the device.\n",
    "    \"\"\"\n",
    "    model = init_specific_model(model_type, img_size, latent_dim).to(device)\n",
    "    # works with state_dict to make it independent of the file structure\n",
    "    model.load_state_dict(torch.load(path_to_model), strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def numpy_serialize(obj):\n",
    "    if type(obj).__module__ == np.__name__:\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj.item()\n",
    "    raise TypeError('Unknown type:', type(obj))\n",
    "\n",
    "\n",
    "def save_np_arrays(arrays, directory, filename):\n",
    "    \"\"\"Save dictionary of arrays in json file.\"\"\"\n",
    "    save_metadata(arrays, directory, filename=filename, default=numpy_serialize)\n",
    "\n",
    "\n",
    "def load_np_arrays(directory, filename):\n",
    "    \"\"\"Load dictionary of arrays from json file.\"\"\"\n",
    "    arrays = load_metadata(directory, filename=filename)\n",
    "    return {k: np.array(v) for k, v in arrays.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module containing all vae losses.\n",
    "\"\"\"\n",
    "import abc\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "LOSSES = [\"VAE\", \"betaH\", \"betaB\", \"factor\", \"btcvae\"]\n",
    "RECON_DIST = [\"bernoulli\", \"laplace\", \"gaussian\"]\n",
    "\n",
    "\n",
    "# TO-DO: clean n_data and device\n",
    "def get_loss_f(loss_name, **kwargs_parse):\n",
    "    \"\"\"Return the correct loss function given the argparse arguments.\"\"\"\n",
    "    kwargs_all = dict(rec_dist=kwargs_parse[\"rec_dist\"],\n",
    "                      steps_anneal=kwargs_parse[\"reg_anneal\"])\n",
    "    if loss_name == \"betaH\":\n",
    "        return BetaHLoss(beta=kwargs_parse[\"betaH_B\"], **kwargs_all)\n",
    "    elif loss_name == \"VAE\":\n",
    "        return BetaHLoss(beta=1, **kwargs_all)\n",
    "    elif loss_name == \"betaB\":\n",
    "        return BetaBLoss(C_init=kwargs_parse[\"betaB_initC\"],\n",
    "                         C_fin=kwargs_parse[\"betaB_finC\"],\n",
    "                         gamma=kwargs_parse[\"betaB_G\"],\n",
    "                         **kwargs_all)\n",
    "\n",
    "    else:\n",
    "        assert loss_name not in LOSSES\n",
    "        raise ValueError(\"Uknown loss : {}\".format(loss_name))\n",
    "\n",
    "\n",
    "class BaseLoss(abc.ABC):\n",
    "    \"\"\"\n",
    "    Base class for losses.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    record_loss_every: int, optional\n",
    "        Every how many steps to recorsd the loss.\n",
    "\n",
    "    rec_dist: {\"bernoulli\", \"gaussian\", \"laplace\"}, optional\n",
    "        Reconstruction distribution istribution of the likelihood on the each pixel.\n",
    "        Implicitely defines the reconstruction loss. Bernoulli corresponds to a\n",
    "        binary cross entropy (bse), Gaussian corresponds to MSE, Laplace\n",
    "        corresponds to L1.\n",
    "\n",
    "    steps_anneal: nool, optional\n",
    "        Number of annealing steps where gradually adding the regularisation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, record_loss_every=50, rec_dist=\"bernoulli\", steps_anneal=0):\n",
    "        self.n_train_steps = 0\n",
    "        self.record_loss_every = record_loss_every\n",
    "        self.rec_dist = rec_dist\n",
    "        self.steps_anneal = steps_anneal\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculates loss for a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : torch.Tensor\n",
    "            Input data (e.g. batch of images). Shape : (batch_size, n_chan,\n",
    "            height, width).\n",
    "\n",
    "        recon_data : torch.Tensor\n",
    "            Reconstructed data. Shape : (batch_size, n_chan, height, width).\n",
    "\n",
    "        latent_dist : tuple of torch.tensor\n",
    "            sufficient statistics of the latent dimension. E.g. for gaussian\n",
    "            (mean, log_var) each of shape : (batch_size, latent_dim).\n",
    "\n",
    "        is_train : bool\n",
    "            Whether currently in train mode.\n",
    "\n",
    "        storer : dict\n",
    "            Dictionary in which to store important variables for vizualisation.\n",
    "\n",
    "        kwargs:\n",
    "            Loss specific arguments\n",
    "        \"\"\"\n",
    "\n",
    "    def _pre_call(self, is_train, storer):\n",
    "        if is_train:\n",
    "            self.n_train_steps += 1\n",
    "\n",
    "        if not is_train or self.n_train_steps % self.record_loss_every == 1:\n",
    "            storer = storer\n",
    "        else:\n",
    "            storer = None\n",
    "\n",
    "        return storer\n",
    "\n",
    "\n",
    "class BetaHLoss(BaseLoss):\n",
    "    \"\"\"\n",
    "    Compute the Beta-VAE loss as in [1]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : float, optional\n",
    "        Weight of the kl divergence.\n",
    "\n",
    "    kwargs:\n",
    "        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] Higgins, Irina, et al. \"beta-vae: Learning basic visual concepts with\n",
    "        a constrained variational framework.\" (2016).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n",
    "        storer = self._pre_call(is_train, storer)\n",
    "\n",
    "        rec_loss = _reconstruction_loss(data, recon_data,\n",
    "                                        storer=storer,\n",
    "                                        distribution=self.rec_dist)\n",
    "        kl_loss = _kl_normal_loss(*latent_dist, storer)\n",
    "        anneal_reg = (linear_annealing(0, 1, self.n_train_steps, self.steps_anneal)\n",
    "                      if is_train else 1)\n",
    "        loss = rec_loss + anneal_reg * (self.beta * kl_loss)\n",
    "\n",
    "        if storer is not None:\n",
    "            storer['loss'].append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BetaBLoss(BaseLoss):\n",
    "    \"\"\"\n",
    "    Compute the Beta-VAE loss as in [1]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    C_init : float, optional\n",
    "        Starting annealed capacity C.\n",
    "\n",
    "    C_fin : float, optional\n",
    "        Final annealed capacity C.\n",
    "\n",
    "    gamma : float, optional\n",
    "        Weight of the KL divergence term.\n",
    "\n",
    "    kwargs:\n",
    "        Additional arguments for `BaseLoss`, e.g. rec_dist`.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "        $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C_init=0., C_fin=20., gamma=100., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.C_init = C_init\n",
    "        self.C_fin = C_fin\n",
    "\n",
    "    def __call__(self, data, recon_data, latent_dist, is_train, storer, **kwargs):\n",
    "        storer = self._pre_call(is_train, storer)\n",
    "\n",
    "        rec_loss = _reconstruction_loss(data, recon_data,\n",
    "                                        storer=storer,\n",
    "                                        distribution=self.rec_dist)\n",
    "        kl_loss = _kl_normal_loss(*latent_dist, storer)\n",
    "\n",
    "        C = (linear_annealing(self.C_init, self.C_fin, self.n_train_steps, self.steps_anneal)\n",
    "             if is_train else self.C_fin)\n",
    "\n",
    "        loss = rec_loss + self.gamma * (kl_loss - C).abs()\n",
    "\n",
    "        if storer is not None:\n",
    "            storer['loss'].append(loss.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def _reconstruction_loss(data, recon_data, distribution=\"bernoulli\", storer=None):\n",
    "    \"\"\"\n",
    "    Calculates the per image reconstruction loss for a batch of data. I.e. negative\n",
    "    log likelihood.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : torch.Tensor\n",
    "        Input data (e.g. batch of images). Shape : (batch_size, n_chan,\n",
    "        height, width).\n",
    "\n",
    "    recon_data : torch.Tensor\n",
    "        Reconstructed data. Shape : (batch_size, n_chan, height, width).\n",
    "\n",
    "    distribution : {\"bernoulli\", \"gaussian\", \"laplace\"}\n",
    "        Distribution of the likelihood on the each pixel. Implicitely defines the\n",
    "        loss Bernoulli corresponds to a binary cross entropy (bse) loss and is the\n",
    "        most commonly used. It has the issue that it doesn't penalize the same\n",
    "        way (0.1,0.2) and (0.4,0.5), which might not be optimal. Gaussian\n",
    "        distribution corresponds to MSE, and is sometimes used, but hard to train\n",
    "        ecause it ends up focusing only a few pixels that are very wrong. Laplace\n",
    "        distribution corresponds to L1 solves partially the issue of MSE.\n",
    "\n",
    "    storer : dict\n",
    "        Dictionary in which to store important variables for vizualisation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : torch.Tensor\n",
    "        Per image cross entropy (i.e. normalized per batch but not pixel and\n",
    "        channel)\n",
    "    \"\"\"\n",
    "    batch_size, n_chan, height, width = recon_data.size()\n",
    "    is_colored = n_chan == 3\n",
    "\n",
    "    if distribution == \"bernoulli\":\n",
    "        loss = F.binary_cross_entropy(recon_data, data, reduction=\"sum\")\n",
    "    elif distribution == \"gaussian\":\n",
    "        # loss in [0,255] space but normalized by 255 to not be too big\n",
    "        loss = F.mse_loss(recon_data * 255, data * 255, reduction=\"sum\") / 255\n",
    "    elif distribution == \"laplace\":\n",
    "        # loss in [0,255] space but normalized by 255 to not be too big but\n",
    "        # multiply by 255 and divide 255, is the same as not doing anything for L1\n",
    "        loss = F.l1_loss(recon_data, data, reduction=\"sum\")\n",
    "        loss = loss * 3  # emperical value to give similar values than bernoulli => use same hyperparam\n",
    "        loss = loss * (loss != 0)  # masking to avoid nan\n",
    "    else:\n",
    "        assert distribution not in RECON_DIST\n",
    "        raise ValueError(\"Unkown distribution: {}\".format(distribution))\n",
    "\n",
    "    loss = loss / batch_size\n",
    "\n",
    "    if storer is not None:\n",
    "        storer['recon_loss'].append(loss.item())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _kl_normal_loss(mean, logvar, storer=None):\n",
    "    \"\"\"\n",
    "    Calculates the KL divergence between a normal distribution\n",
    "    with diagonal covariance and a unit normal distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : torch.Tensor\n",
    "        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n",
    "        D is dimension of distribution.\n",
    "\n",
    "    logvar : torch.Tensor\n",
    "        Diagonal log variance of the normal distribution. Shape (batch_size,\n",
    "        latent_dim)\n",
    "\n",
    "    storer : dict\n",
    "        Dictionary in which to store important variables for vizualisation.\n",
    "    \"\"\"\n",
    "    latent_dim = mean.size(1)\n",
    "    # batch mean of kl for each latent dimension\n",
    "    latent_kl = 0.5 * (-1 - logvar + mean.pow(2) + logvar.exp()).mean(dim=0)\n",
    "    total_kl = latent_kl.sum()\n",
    "\n",
    "    if storer is not None:\n",
    "        storer['kl_loss'].append(total_kl.item())\n",
    "        for i in range(latent_dim):\n",
    "            storer['kl_loss_' + str(i)].append(latent_kl[i].item())\n",
    "\n",
    "    return total_kl\n",
    "\n",
    "\n",
    "def _permute_dims(latent_sample):\n",
    "    \"\"\"\n",
    "    Implementation of Algorithm 1 in ref [1]. Randomly permutes the sample from\n",
    "    q(z) (latent_dist) across the batch for each of the latent dimensions (mean\n",
    "    and log_var).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latent_sample: torch.Tensor\n",
    "        sample from the latent dimension using the reparameterisation trick\n",
    "        shape : (batch_size, latent_dim).\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "        [1] Kim, Hyunjik, and Andriy Mnih. \"Disentangling by factorising.\"\n",
    "        arXiv preprint arXiv:1802.05983 (2018).\n",
    "\n",
    "    \"\"\"\n",
    "    perm = torch.zeros_like(latent_sample)\n",
    "    batch_size, dim_z = perm.size()\n",
    "\n",
    "    for z in range(dim_z):\n",
    "        pi = torch.randperm(batch_size).to(latent_sample.device)\n",
    "        perm[:, z] = latent_sample[pi, z]\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def linear_annealing(init, fin, step, annealing_steps):\n",
    "    \"\"\"Linear annealing of a parameter.\"\"\"\n",
    "    if annealing_steps == 0:\n",
    "        return fin\n",
    "    assert fin > init\n",
    "    delta = fin - init\n",
    "    annealed = min(init + delta * step / annealing_steps, fin)\n",
    "    return annealed\n",
    "\n",
    "\n",
    "# Batch TC specific\n",
    "# TO-DO: test if mss is better!\n",
    "def _get_log_pz_qz_prodzi_qzCx(latent_sample, latent_dist, n_data, is_mss=True):\n",
    "    batch_size, hidden_dim = latent_sample.shape\n",
    "\n",
    "    # calculate log q(z|x)\n",
    "    log_q_zCx = log_density_gaussian(latent_sample, *latent_dist).sum(dim=1)\n",
    "\n",
    "    # calculate log p(z)\n",
    "    # mean and log var is 0\n",
    "    zeros = torch.zeros_like(latent_sample)\n",
    "    log_pz = log_density_gaussian(latent_sample, zeros, zeros).sum(1)\n",
    "\n",
    "    mat_log_qz = matrix_log_density_gaussian(latent_sample, *latent_dist)\n",
    "\n",
    "    if is_mss:\n",
    "        # use stratification\n",
    "        log_iw_mat = log_importance_weight_matrix(batch_size, n_data).to(latent_sample.device)\n",
    "        mat_log_qz = mat_log_qz + log_iw_mat.view(batch_size, batch_size, 1)\n",
    "\n",
    "    log_qz = torch.logsumexp(mat_log_qz.sum(2), dim=1, keepdim=False)\n",
    "    log_prod_qzi = torch.logsumexp(mat_log_qz, dim=1, keepdim=False).sum(1)\n",
    "\n",
    "    return log_pz, log_qz, log_prod_qzi, log_q_zCx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# ALL encoders should be called Enccoder<Model>\n",
    "def get_encoder(model_type):\n",
    "    model_type = model_type.lower().capitalize()\n",
    "    return eval(\"Encoder{}\".format(model_type))\n",
    "\n",
    "\n",
    "class EncoderBurgess(nn.Module):\n",
    "    def __init__(self, img_size,\n",
    "                 latent_dim=10):\n",
    "        r\"\"\"Encoder of the model proposed in [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_size : tuple of ints\n",
    "            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n",
    "\n",
    "        latent_dim : int\n",
    "            Dimensionality of latent output.\n",
    "\n",
    "        Model Architecture (transposed for decoder)\n",
    "        ------------\n",
    "        - 4 convolutional layers (each with 32 channels), (4 x 4 kernel), (stride of 2)\n",
    "        - 2 fully connected layers (each of 256 units)\n",
    "        - Latent distribution:\n",
    "            - 1 fully connected layer of 20 units (log variance and mean for 10 Gaussians)\n",
    "\n",
    "        References:\n",
    "            [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "            $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "        \"\"\"\n",
    "        super(EncoderBurgess, self).__init__()\n",
    "\n",
    "        # Layer parameters\n",
    "        hid_channels = 32\n",
    "        kernel_size = 4\n",
    "        hidden_dim = 256\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = img_size\n",
    "        # Shape required to start transpose convs\n",
    "        self.reshape = (hid_channels, kernel_size, kernel_size)\n",
    "        n_chan = self.img_size[0]\n",
    "\n",
    "        # Convolutional layers\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "        self.conv1 = nn.Conv2d(n_chan, hid_channels, kernel_size, **cnn_kwargs)\n",
    "        self.conv2 = nn.Conv2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n",
    "        self.conv3 = nn.Conv2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        # If input image is 64x64 do fourth convolution\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            self.conv_64 = nn.Conv2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.lin1 = nn.Linear(np.product(self.reshape), hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Fully connected layers for mean and variance\n",
    "        self.mu_logvar_gen = nn.Linear(hidden_dim, self.latent_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Convolutional layers with ReLu activations\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            x = torch.relu(self.conv_64(x))\n",
    "\n",
    "        # Fully connected layers with ReLu activations\n",
    "        x = x.view((batch_size, -1))\n",
    "        x = torch.relu(self.lin1(x))\n",
    "        x = torch.relu(self.lin2(x))\n",
    "\n",
    "        # Fully connected layer for log variance and mean\n",
    "        # Log std-dev in paper (bear in mind)\n",
    "        mu_logvar = self.mu_logvar_gen(x)\n",
    "        mu, logvar = mu_logvar.view(-1, self.latent_dim, 2).unbind(-1)\n",
    "\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module containing the decoders.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# ALL decoders should be called Decoder<Model>\n",
    "def get_decoder(model_type):\n",
    "    model_type = model_type.lower().capitalize()\n",
    "    return eval(\"Decoder{}\".format(model_type))\n",
    "\n",
    "\n",
    "class DecoderBurgess(nn.Module):\n",
    "    def __init__(self, img_size,\n",
    "                 latent_dim=10):\n",
    "        r\"\"\"Decoder of the model proposed in [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_size : tuple of ints\n",
    "            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n",
    "\n",
    "        latent_dim : int\n",
    "            Dimensionality of latent output.\n",
    "\n",
    "        Model Architecture (transposed for decoder)\n",
    "        ------------\n",
    "        - 4 convolutional layers (each with 32 channels), (4 x 4 kernel), (stride of 2)\n",
    "        - 2 fully connected layers (each of 256 units)\n",
    "        - Latent distribution:\n",
    "            - 1 fully connected layer of 20 units (log variance and mean for 10 Gaussians)\n",
    "\n",
    "        References:\n",
    "            [1] Burgess, Christopher P., et al. \"Understanding disentangling in\n",
    "            $\\beta$-VAE.\" arXiv preprint arXiv:1804.03599 (2018).\n",
    "        \"\"\"\n",
    "        super(DecoderBurgess, self).__init__()\n",
    "\n",
    "        # Layer parameters\n",
    "        hid_channels = 32\n",
    "        kernel_size = 4\n",
    "        hidden_dim = 256\n",
    "        self.img_size = img_size\n",
    "        # Shape required to start transpose convs\n",
    "        self.reshape = (hid_channels, kernel_size, kernel_size)\n",
    "        n_chan = self.img_size[0]\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.lin1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, np.product(self.reshape))\n",
    "\n",
    "        # Convolutional layers\n",
    "        cnn_kwargs = dict(stride=2, padding=1)\n",
    "        # If input image is 64x64 do fourth convolution\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            self.convT_64 = nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n",
    "        self.convT2 = nn.ConvTranspose2d(hid_channels, hid_channels, kernel_size, **cnn_kwargs)\n",
    "        self.convT3 = nn.ConvTranspose2d(hid_channels, n_chan, kernel_size, **cnn_kwargs)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        # Fully connected layers with ReLu activations\n",
    "        x = torch.relu(self.lin1(z))\n",
    "        x = torch.relu(self.lin2(x))\n",
    "        x = torch.relu(self.lin3(x))\n",
    "        x = x.view(batch_size, *self.reshape)\n",
    "\n",
    "        # Convolutional layers with ReLu activations\n",
    "        if self.img_size[1] == self.img_size[2] == 64:\n",
    "            x = torch.relu(self.convT_64(x))\n",
    "        x = torch.relu(self.convT1(x))\n",
    "        x = torch.relu(self.convT2(x))\n",
    "        # Sigmoid activation for final conv layer\n",
    "        x = torch.sigmoid(self.convT3(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init specific model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module containing the main VAE class.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# from .encoders import get_encoder\n",
    "# from .decoders import get_decoder\n",
    "\n",
    "MODELS = [\"Burgess\"]\n",
    "\n",
    "\n",
    "def init_specific_model(model_type, img_size, latent_dim):\n",
    "    \"\"\"Return an instance of a VAE with encoder and decoder from `model_type`.\"\"\"\n",
    "    model_type = model_type.lower().capitalize()\n",
    "    if model_type not in MODELS:\n",
    "        err = \"Unkown model_type={}. Possible values: {}\"\n",
    "        raise ValueError(err.format(model_type, MODELS))\n",
    "\n",
    "    encoder = get_encoder(model_type)\n",
    "    decoder = get_decoder(model_type)\n",
    "    model = VAE(img_size, encoder, decoder, latent_dim)\n",
    "    model.model_type = model_type  # store to help reloading\n",
    "    return model\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_size, encoder, decoder, latent_dim):\n",
    "        \"\"\"\n",
    "        Class which defines model and forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_size : tuple of ints\n",
    "            Size of images. E.g. (1, 32, 32) or (3, 64, 64).\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        if list(img_size[1:]) not in [[32, 32], [64, 64]]:\n",
    "            raise RuntimeError(\"{} sized images not supported. Only (None, 32, 32) and (None, 64, 64) supported. Build your own architecture or reshape images!\".format(img_size))\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = img_size\n",
    "        self.num_pixels = self.img_size[1] * self.img_size[2]\n",
    "        self.encoder = encoder(img_size, self.latent_dim)\n",
    "        self.decoder = decoder(img_size, self.latent_dim)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Samples from a normal distribution using the reparameterization trick.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : torch.Tensor\n",
    "            Mean of the normal distribution. Shape (batch_size, latent_dim)\n",
    "\n",
    "        logvar : torch.Tensor\n",
    "            Diagonal log variance of the normal distribution. Shape (batch_size,\n",
    "            latent_dim)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + std * eps\n",
    "        else:\n",
    "            # Reconstruction mode\n",
    "            return mean\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
    "        \"\"\"\n",
    "        latent_dist = self.encoder(x)\n",
    "        latent_sample = self.reparameterize(*latent_dist)\n",
    "        reconstruct = self.decoder(latent_sample)\n",
    "        return reconstruct, latent_dist, latent_sample\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def sample_latent(self, x):\n",
    "        \"\"\"\n",
    "        Returns a sample from the latent distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
    "        \"\"\"\n",
    "        latent_dist = self.encoder(x)\n",
    "        latent_sample = self.reparameterize(*latent_dist)\n",
    "        return latent_sample\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import logging\n",
    "import os\n",
    "from timeit import default_timer\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_LOSSES_LOGFILE = \"train_losses.log\"\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    \"\"\"\n",
    "    Class to handle training of model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: disvae.vae.VAE\n",
    "\n",
    "    optimizer: torch.optim.Optimizer\n",
    "\n",
    "    loss_f: disvae.models.BaseLoss\n",
    "        Loss function.\n",
    "\n",
    "    device: torch.device, optional\n",
    "        Device on which to run the code.\n",
    "\n",
    "    logger: logging.Logger, optional\n",
    "        Logger.\n",
    "\n",
    "    save_dir : str, optional\n",
    "        Directory for saving logs.\n",
    "\n",
    "    gif_visualizer : viz.Visualizer, optional\n",
    "        Gif Visualizer that should return samples at every epochs.\n",
    "\n",
    "    is_progress_bar: bool, optional\n",
    "        Whether to use a progress bar for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss_f,\n",
    "                 device=torch.device(\"cpu\"),\n",
    "                 logger=logging.getLogger(__name__),\n",
    "                 save_dir=\"results\",\n",
    "                 gif_visualizer=None,\n",
    "                 is_progress_bar=True):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.loss_f = loss_f\n",
    "        self.optimizer = optimizer\n",
    "        self.save_dir = save_dir\n",
    "        self.is_progress_bar = is_progress_bar\n",
    "        self.logger = logger\n",
    "        self.losses_logger = LossesLogger(os.path.join(self.save_dir, TRAIN_LOSSES_LOGFILE))\n",
    "        self.gif_visualizer = gif_visualizer\n",
    "        self.logger.info(\"Training Device: {}\".format(self.device))\n",
    "\n",
    "    def __call__(self, data_loader,\n",
    "                 epochs=10,\n",
    "                 checkpoint_every=10):\n",
    "        \"\"\"\n",
    "        Trains the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader: torch.utils.data.DataLoader\n",
    "\n",
    "        epochs: int, optional\n",
    "            Number of epochs to train the model for.\n",
    "\n",
    "        checkpoint_every: int, optional\n",
    "            Save a checkpoint of the trained model every n epoch.\n",
    "        \"\"\"\n",
    "        start = default_timer()\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            storer = defaultdict(list)\n",
    "            mean_epoch_loss = self._train_epoch(data_loader, storer, epoch)\n",
    "            self.logger.info('Epoch: {} Average loss per image: {:.2f}'.format(epoch + 1,\n",
    "                                                                               mean_epoch_loss))\n",
    "            self.losses_logger.log(epoch, storer)\n",
    "\n",
    "            if self.gif_visualizer is not None:\n",
    "                self.gif_visualizer()\n",
    "\n",
    "            if epoch % checkpoint_every == 0:\n",
    "                save_model(self.model, self.save_dir,\n",
    "                           filename=\"model-{}.pt\".format(epoch))\n",
    "\n",
    "        if self.gif_visualizer is not None:\n",
    "            self.gif_visualizer.save_reset()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        delta_time = (default_timer() - start) / 60\n",
    "        self.logger.info('Finished training after {:.1f} min.'.format(delta_time))\n",
    "\n",
    "    def _train_epoch(self, data_loader, storer, epoch):\n",
    "        \"\"\"\n",
    "        Trains the model for one epoch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader: torch.utils.data.DataLoader\n",
    "\n",
    "        storer: dict\n",
    "            Dictionary in which to store important variables for vizualisation.\n",
    "\n",
    "        epoch: int\n",
    "            Epoch number\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        mean_epoch_loss: float\n",
    "            Mean loss per image\n",
    "        \"\"\"\n",
    "        epoch_loss = 0.\n",
    "        kwargs = dict(desc=\"Epoch {}\".format(epoch + 1), leave=False,\n",
    "                      disable=not self.is_progress_bar)\n",
    "        with trange(len(data_loader), **kwargs) as t:\n",
    "            for _, (data, _) in enumerate(data_loader):\n",
    "                iter_loss = self._train_iteration(data, storer)\n",
    "                epoch_loss += iter_loss\n",
    "\n",
    "                t.set_postfix(loss=iter_loss)\n",
    "                t.update()\n",
    "\n",
    "        mean_epoch_loss = epoch_loss / len(data_loader)\n",
    "        return mean_epoch_loss\n",
    "\n",
    "    def _train_iteration(self, data, storer):\n",
    "        \"\"\"\n",
    "        Trains the model for one iteration on a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: torch.Tensor\n",
    "            A batch of data. Shape : (batch_size, channel, height, width).\n",
    "\n",
    "        storer: dict\n",
    "            Dictionary in which to store important variables for vizualisation.\n",
    "        \"\"\"\n",
    "        batch_size, channel, height, width = data.size()\n",
    "        data = data.to(self.device)\n",
    "\n",
    "        try:\n",
    "            recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "            loss = self.loss_f(data, recon_batch, latent_dist, self.model.training,\n",
    "                               storer, latent_sample=latent_sample)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        except ValueError:\n",
    "            # for losses that use multiple optimizers (e.g. Factor)\n",
    "            loss = self.loss_f.call_optimize(data, self.model, self.optimizer, storer)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "class LossesLogger(object):\n",
    "    \"\"\"Class definition for objects to write data to log files in a\n",
    "    form which is then easy to be plotted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path_name):\n",
    "        \"\"\" Create a logger to store information for plotting. \"\"\"\n",
    "        if os.path.isfile(file_path_name):\n",
    "            os.remove(file_path_name)\n",
    "\n",
    "        self.logger = logging.getLogger(\"losses_logger\")\n",
    "        self.logger.setLevel(1)  # always store\n",
    "        file_handler = logging.FileHandler(file_path_name)\n",
    "        file_handler.setLevel(1)\n",
    "        self.logger.addHandler(file_handler)\n",
    "\n",
    "        header = \",\".join([\"Epoch\", \"Loss\", \"Value\"])\n",
    "        self.logger.debug(header)\n",
    "\n",
    "    def log(self, epoch, losses_storer):\n",
    "        \"\"\"Write to the log file \"\"\"\n",
    "        for k, v in losses_storer.items():\n",
    "            log_string = \",\".join(str(item) for item in [epoch, k, mean(v)])\n",
    "            self.logger.debug(log_string)\n",
    "\n",
    "\n",
    "# HELPERS\n",
    "def mean(l):\n",
    "    \"\"\"Compute the mean of a list\"\"\"\n",
    "    return sum(l) / len(l)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import math\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from timeit import default_timer\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "TEST_LOSSES_FILE = \"test_losses.log\"\n",
    "METRICS_FILENAME = \"metrics.log\"\n",
    "METRIC_HELPERS_FILE = \"metric_helpers.pth\"\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, loss_f,\n",
    "                 device=torch.device(\"cpu\"),\n",
    "                 logger=logging.getLogger(__name__),\n",
    "                 save_dir=\"results\",\n",
    "                 is_progress_bar=True):\n",
    "\n",
    "        self.device = device\n",
    "        self.loss_f = loss_f\n",
    "        self.model = model.to(self.device)\n",
    "        self.logger = logger\n",
    "        self.save_dir = save_dir\n",
    "        self.is_progress_bar = is_progress_bar\n",
    "        self.logger.info(\"Testing Device: {}\".format(self.device))\n",
    "\n",
    "    def __call__(self, data_loader, is_metrics=False, is_losses=True):\n",
    "        \"\"\"Compute all test losses.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data_loader: torch.utils.data.DataLoader\n",
    "\n",
    "        is_metrics: bool, optional\n",
    "            Whether to compute and store the disentangling metrics.\n",
    "\n",
    "        is_losses: bool, optional\n",
    "            Whether to compute and store the test losses.\n",
    "        \"\"\"\n",
    "        start = default_timer()\n",
    "        is_still_training = self.model.training\n",
    "        self.model.eval()\n",
    "\n",
    "        metric, losses = None, None\n",
    "        if is_metrics:\n",
    "            self.logger.info('Computing metrics...')\n",
    "            metrics = self.compute_metrics(data_loader)\n",
    "            self.logger.info('Losses: {}'.format(metrics))\n",
    "            save_metadata(metrics, self.save_dir, filename=METRICS_FILENAME)\n",
    "\n",
    "        if is_losses:\n",
    "            self.logger.info('Computing losses...')\n",
    "            losses = self.compute_losses(data_loader)\n",
    "            self.logger.info('Losses: {}'.format(losses))\n",
    "            save_metadata(losses, self.save_dir, filename=TEST_LOSSES_FILE)\n",
    "\n",
    "        if is_still_training:\n",
    "            self.model.train()\n",
    "\n",
    "        self.logger.info('Finished evaluating after {:.1f} min.'.format((default_timer() - start) / 60))\n",
    "\n",
    "        return metric, losses\n",
    "\n",
    "    def compute_losses(self, dataloader):\n",
    "\n",
    "        storer = defaultdict(list)\n",
    "        for data, _ in tqdm(dataloader, leave=False, disable=not self.is_progress_bar):\n",
    "            data = data.to(self.device)\n",
    "\n",
    "            try:\n",
    "                recon_batch, latent_dist, latent_sample = self.model(data)\n",
    "                _ = self.loss_f(data, recon_batch, latent_dist, self.model.training,\n",
    "                                storer, latent_sample=latent_sample)\n",
    "            except ValueError:\n",
    "                # for losses that use multiple optimizers (e.g. Factor)\n",
    "                _ = self.loss_f.call_optimize(data, self.model, None, storer)\n",
    "\n",
    "            losses = {k: sum(v) / len(dataloader) for k, v in storer.items()}\n",
    "            return losses\n",
    "\n",
    "    def compute_metrics(self, dataloader):\n",
    "\n",
    "        try:\n",
    "            lat_sizes = dataloader.dataset.lat_sizes\n",
    "            lat_names = dataloader.dataset.lat_names\n",
    "        except AttributeError:\n",
    "            raise ValueError(\"Dataset needs to have known true factors of variations to compute the metric. This does not seem to be the case for {}\".format(type(dataloader.__dict__[\"dataset\"]).__name__))\n",
    "\n",
    "        self.logger.info(\"Computing the empirical distribution q(z|x).\")\n",
    "        samples_zCx, params_zCx = self._compute_q_zCx(dataloader)\n",
    "        len_dataset, latent_dim = samples_zCx.shape\n",
    "\n",
    "        self.logger.info(\"Estimating the marginal entropy.\")\n",
    "        # marginal entropy H(z_j)\n",
    "        H_z = self._estimate_latent_entropies(samples_zCx, params_zCx)\n",
    "\n",
    "        # conditional entropy H(z|v)\n",
    "        samples_zCx = samples_zCx.view(*lat_sizes, latent_dim)\n",
    "        params_zCx = tuple(p.view(*lat_sizes, latent_dim) for p in params_zCx)\n",
    "        H_zCv = self._estimate_H_zCv(samples_zCx, params_zCx, lat_sizes, lat_names)\n",
    "\n",
    "        H_z = H_z.cpu()\n",
    "        H_zCv = H_zCv.cpu()\n",
    "\n",
    "        # I[z_j;v_k] = E[log \\sum_x q(z_j|x)p(x|v_k)] + H[z_j] = - H[z_j|v_k] + H[z_j]\n",
    "        mut_info = - H_zCv + H_z\n",
    "        sorted_mut_info = torch.sort(mut_info, dim=1, descending=True)[0].clamp(min=0)\n",
    "\n",
    "        metric_helpers = {'marginal_entropies': H_z, 'cond_entropies': H_zCv}\n",
    "        mig = self._mutual_information_gap(sorted_mut_info, lat_sizes, storer=metric_helpers)\n",
    "        aam = self._axis_aligned_metric(sorted_mut_info, storer=metric_helpers)\n",
    "\n",
    "        metrics = {'MIG': mig.item(), 'AAM': aam.item()}\n",
    "        torch.save(metric_helpers, os.path.join(self.save_dir, METRIC_HELPERS_FILE))\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _mutual_information_gap(self, sorted_mut_info, lat_sizes, storer=None):\n",
    "        \"\"\"Compute the mutual information gap as in [1].\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "           [1] Chen, Tian Qi, et al. \"Isolating sources of disentanglement in variational\n",
    "           autoencoders.\" Advances in Neural Information Processing Systems. 2018.\n",
    "        \"\"\"\n",
    "        # difference between the largest and second largest mutual info\n",
    "        delta_mut_info = sorted_mut_info[:, 0] - sorted_mut_info[:, 1]\n",
    "        # NOTE: currently only works if balanced dataset for every factor of variation\n",
    "        # then H(v_k) = - |V_k|/|V_k| log(1/|V_k|) = log(|V_k|)\n",
    "        H_v = torch.from_numpy(lat_sizes).float().log()\n",
    "        mig_k = delta_mut_info / H_v\n",
    "        mig = mig_k.mean()  # mean over factor of variations\n",
    "\n",
    "        if storer is not None:\n",
    "            storer[\"mig_k\"] = mig_k\n",
    "            storer[\"mig\"] = mig\n",
    "\n",
    "        return mig\n",
    "\n",
    "    def _axis_aligned_metric(self, sorted_mut_info, storer=None):\n",
    "        \"\"\"Compute the proposed axis aligned metrics.\"\"\"\n",
    "        numerator = (sorted_mut_info[:, 0] - sorted_mut_info[:, 1:].sum(dim=1)).clamp(min=0)\n",
    "        aam_k = numerator / sorted_mut_info[:, 0]\n",
    "        aam_k[torch.isnan(aam_k)] = 0\n",
    "        aam = aam_k.mean()  # mean over factor of variations\n",
    "\n",
    "        if storer is not None:\n",
    "            storer[\"aam_k\"] = aam_k\n",
    "            storer[\"aam\"] = aam\n",
    "\n",
    "        return aam\n",
    "\n",
    "    def _compute_q_zCx(self, dataloader):\n",
    "        \"\"\"Compute the empiricall disitribution of q(z|x).\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        dataloader: torch.utils.data.DataLoader\n",
    "            Batch data iterator.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        samples_zCx: torch.tensor\n",
    "            Tensor of shape (len_dataset, latent_dim) containing a sample of\n",
    "            q(z|x) for every x in the dataset.\n",
    "\n",
    "        params_zCX: tuple of torch.Tensor\n",
    "            Sufficient statistics q(z|x) for each training example. E.g. for\n",
    "            gaussian (mean, log_var) each of shape : (len_dataset, latent_dim).\n",
    "        \"\"\"\n",
    "        len_dataset = len(dataloader.dataset)\n",
    "        latent_dim = self.model.latent_dim\n",
    "        n_suff_stat = 2\n",
    "\n",
    "        q_zCx = torch.zeros(len_dataset, latent_dim, n_suff_stat, device=self.device)\n",
    "\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for x, label in dataloader:\n",
    "                batch_size = x.size(0)\n",
    "                idcs = slice(n, n + batch_size)\n",
    "                q_zCx[idcs, :, 0], q_zCx[idcs, :, 1] = self.model.encoder(x.to(self.device))\n",
    "                n += batch_size\n",
    "\n",
    "        params_zCX = q_zCx.unbind(-1)\n",
    "        samples_zCx = self.model.reparameterize(*params_zCX)\n",
    "\n",
    "        return samples_zCx, params_zCX\n",
    "\n",
    "    def _estimate_latent_entropies(self, samples_zCx, params_zCX,\n",
    "                                   n_samples=10000):\n",
    "        r\"\"\"Estimate :math:`H(z_j) = E_{q(z_j)} [-log q(z_j)] = E_{p(x)} E_{q(z_j|x)} [-log q(z_j)]`\n",
    "        using the emperical distribution of :math:`p(x)`.\n",
    "\n",
    "        Note\n",
    "        ----\n",
    "        - the expectation over the emperical distributio is: :math:`q(z) = 1/N sum_{n=1}^N q(z|x_n)`.\n",
    "        - we assume that q(z|x) is factorial i.e. :math:`q(z|x) = \\prod_j q(z_j|x)`.\n",
    "        - computes numerically stable NLL: :math:`- log q(z) = log N - logsumexp_n=1^N log q(z|x_n)`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples_zCx: torch.tensor\n",
    "            Tensor of shape (len_dataset, latent_dim) containing a sample of\n",
    "            q(z|x) for every x in the dataset.\n",
    "\n",
    "        params_zCX: tuple of torch.Tensor\n",
    "            Sufficient statistics q(z|x) for each training example. E.g. for\n",
    "            gaussian (mean, log_var) each of shape : (len_dataset, latent_dim).\n",
    "\n",
    "        n_samples: int, optional\n",
    "            Number of samples to use to estimate the entropies.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        H_z: torch.Tensor\n",
    "            Tensor of shape (latent_dim) containing the marginal entropies H(z_j)\n",
    "        \"\"\"\n",
    "        len_dataset, latent_dim = samples_zCx.shape\n",
    "        device = samples_zCx.device\n",
    "        H_z = torch.zeros(latent_dim, device=device)\n",
    "\n",
    "        # sample from p(x)\n",
    "        samples_x = torch.randperm(len_dataset, device=device)[:n_samples]\n",
    "        # sample from p(z|x)\n",
    "        samples_zCx = samples_zCx.index_select(0, samples_x).view(latent_dim, n_samples)\n",
    "\n",
    "        mini_batch_size = 10\n",
    "        samples_zCx = samples_zCx.expand(len_dataset, latent_dim, n_samples)\n",
    "        mean = params_zCX[0].unsqueeze(-1).expand(len_dataset, latent_dim, n_samples)\n",
    "        log_var = params_zCX[1].unsqueeze(-1).expand(len_dataset, latent_dim, n_samples)\n",
    "        log_N = math.log(len_dataset)\n",
    "        with trange(n_samples, leave=False, disable=self.is_progress_bar) as t:\n",
    "            for k in range(0, n_samples, mini_batch_size):\n",
    "                # log q(z_j|x) for n_samples\n",
    "                idcs = slice(k, k + mini_batch_size)\n",
    "                log_q_zCx = log_density_gaussian(samples_zCx[..., idcs],\n",
    "                                                 mean[..., idcs],\n",
    "                                                 log_var[..., idcs])\n",
    "                # numerically stable log q(z_j) for n_samples:\n",
    "                # log q(z_j) = -log N + logsumexp_{n=1}^N log q(z_j|x_n)\n",
    "                # As we don't know q(z) we appoximate it with the monte carlo\n",
    "                # expectation of q(z_j|x_n) over x. => fix a single z and look at\n",
    "                # proba for every x to generate it. n_samples is not used here !\n",
    "                log_q_z = -log_N + torch.logsumexp(log_q_zCx, dim=0, keepdim=False)\n",
    "                # H(z_j) = E_{z_j}[- log q(z_j)]\n",
    "                # mean over n_samples (i.e. dimesnion 1 because already summed over 0).\n",
    "                H_z += (-log_q_z).sum(1)\n",
    "\n",
    "                t.update(mini_batch_size)\n",
    "\n",
    "        H_z /= n_samples\n",
    "\n",
    "        return H_z\n",
    "\n",
    "    def _estimate_H_zCv(self, samples_zCx, params_zCx, lat_sizes, lat_names):\n",
    "        \"\"\"Estimate conditional entropies :math:`H[z|v]`.\"\"\"\n",
    "        latent_dim = samples_zCx.size(-1)\n",
    "        len_dataset = reduce((lambda x, y: x * y), lat_sizes)\n",
    "        H_zCv = torch.zeros(len(lat_sizes), latent_dim, device=self.device)\n",
    "        for i_fac_var, (lat_size, lat_name) in enumerate(zip(lat_sizes, lat_names)):\n",
    "            idcs = [slice(None)] * len(lat_sizes)\n",
    "            for i in range(lat_size):\n",
    "                self.logger.info(\"Estimating conditional entropies for the {}th value of {}.\".format(i, lat_name))\n",
    "                idcs[i_fac_var] = i\n",
    "                # samples from q(z,x|v)\n",
    "                samples_zxCv = samples_zCx[idcs].contiguous().view(len_dataset // lat_size,\n",
    "                                                                   latent_dim)\n",
    "                params_zxCv = tuple(p[idcs].contiguous().view(len_dataset // lat_size, latent_dim)\n",
    "                                    for p in params_zCx)\n",
    "\n",
    "                H_zCv[i_fac_var] += self._estimate_latent_entropies(samples_zxCv, params_zxCv\n",
    "                                                                    ) / lat_size\n",
    "        return H_zCv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(args_to_parse):\n",
    "\n",
    "    default_config = get_config_section([CONFIG_FILE], \"Custom\")\n",
    "\n",
    "    description = \"PyTorch implementation and evaluation of disentangled Variational AutoEncoders and metrics.\"\n",
    "    parser = argparse.ArgumentParser(description=description,\n",
    "                                     formatter_class=FormatterNoDuplicate)\n",
    "\n",
    "    # General options\n",
    "    general = parser.add_argument_group('General options')\n",
    "    general.add_argument('name', type=str,\n",
    "                         help=\"Name of the model for storing and loading purposes.\")\n",
    "    # general.add_argument('-L', '--log-level', help=\"Logging levels.\",\n",
    "    #                      default=default_config['log_level'], choices=LOG_LEVELS)\n",
    "    general.add_argument('--no-progress-bar', action='store_true',\n",
    "                         default=default_config['no_progress_bar'],\n",
    "                         help='Disables progress bar.')\n",
    "    general.add_argument('--no-cuda', action='store_true',\n",
    "                         default=default_config['no_cuda'],\n",
    "                         help='Disables CUDA training, even when have one.')\n",
    "    general.add_argument('-s', '--seed', type=int, default=default_config['seed'],\n",
    "                         help='Random seed. Can be `None` for stochastic behavior.')\n",
    "\n",
    "    # Learning options\n",
    "    training = parser.add_argument_group('Training specific options')\n",
    "    training.add_argument('--checkpoint-every',\n",
    "                          type=int, default=default_config['checkpoint_every'],\n",
    "                          help='Save a checkpoint of the trained model every n epoch.')\n",
    "    training.add_argument('-d', '--dataset', help=\"Path to training data.\",\n",
    "                          default=default_config['dataset'], choices=DATASETS)\n",
    "    # training.add_argument('-x', '--experiment',\n",
    "    #                       default=default_config['experiment'], choices=EXPERIMENTS,\n",
    "    #                       help='Predefined experiments to run. If not `custom` this will overwrite some other arguments.')\n",
    "    training.add_argument('-e', '--epochs', type=int,\n",
    "                          default=default_config['epochs'],\n",
    "                          help='Maximum number of epochs to run for.')\n",
    "    training.add_argument('-b', '--batch-size', type=int,\n",
    "                          default=default_config['batch_size'],\n",
    "                          help='Batch size for training.')\n",
    "    training.add_argument('--lr', type=float, default=default_config['lr'],\n",
    "                          help='Learning rate.')\n",
    "\n",
    "    # Model Options\n",
    "    model = parser.add_argument_group('Model specfic options')\n",
    "    model.add_argument('-m', '--model-type',\n",
    "                       default=default_config['model'], choices=MODELS,\n",
    "                       help='Type of encoder and decoder to use.')\n",
    "    model.add_argument('-z', '--latent-dim', type=int,\n",
    "                       default=default_config['latent_dim'],\n",
    "                       help='Dimension of the latent variable.')\n",
    "    model.add_argument('-l', '--loss',\n",
    "                       default=default_config['loss'], choices=LOSSES,\n",
    "                       help=\"Type of VAE loss function to use.\")\n",
    "    model.add_argument('-r', '--rec-dist', default=default_config['rec_dist'],\n",
    "                       choices=RECON_DIST,\n",
    "                       help=\"Form of the likelihood ot use for each pixel.\")\n",
    "    model.add_argument('-a', '--reg-anneal', type=float,\n",
    "                       default=default_config['reg_anneal'],\n",
    "                       help=\"Number of annealing steps where gradually adding the regularisation. What is annealed is specific to each loss.\")\n",
    "\n",
    "    # Loss Specific Options\n",
    "    betaH = parser.add_argument_group('BetaH specific parameters')\n",
    "    betaH.add_argument('--betaH-B', type=float,\n",
    "                       default=default_config['betaH_B'],\n",
    "                       help=\"Weight of the KL (beta in the paper).\")\n",
    "\n",
    "    betaB = parser.add_argument_group('BetaB specific parameters')\n",
    "    betaB.add_argument('--betaB-initC', type=float,\n",
    "                       default=default_config['betaB_initC'],\n",
    "                       help=\"Starting annealed capacity.\")\n",
    "    betaB.add_argument('--betaB-finC', type=float,\n",
    "                       default=default_config['betaB_finC'],\n",
    "                       help=\"Final annealed capacity.\")\n",
    "    betaB.add_argument('--betaB-G', type=float,\n",
    "                       default=default_config['betaB_G'],\n",
    "                       help=\"Weight of the KL divergence term (gamma in the paper).\")\n",
    "\n",
    "    # Learning options\n",
    "    evaluation = parser.add_argument_group('Evaluation specific options')\n",
    "    evaluation.add_argument('--is-eval-only', action='store_true',\n",
    "                            default=default_config['is_eval_only'],\n",
    "                            help='Whether to only evaluate using precomputed model `name`.')\n",
    "    evaluation.add_argument('--is-metrics', action='store_true',\n",
    "                            default=default_config['is_metrics'],\n",
    "                            help=\"Whether to compute the disentangled metrcics. Currently only possible with `dsprites` as it is the only dataset with known true factors of variations.\")\n",
    "    evaluation.add_argument('--no-test', action='store_true',\n",
    "                            default=default_config['no_test'],\n",
    "                            help=\"Whether not to compute the test losses.`\")\n",
    "    evaluation.add_argument('--eval-batchsize', type=int,\n",
    "                            default=default_config['eval_batchsize'],\n",
    "                            help='Batch size for evaluation.')\n",
    "\n",
    "    args = parser.parse_args(args_to_parse)\n",
    "\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliana\\Documents\\FGV\\ML\\TrabalhoFinal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "print(current_dir)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"Main train and evaluation function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: argparse.Namespace\n",
    "        Arguments\n",
    "    \"\"\"\n",
    "\n",
    "    set_seed(args.seed)\n",
    "    device = get_device()\n",
    "    exp_dir = os.path.join(RES_DIR, args.name)\n",
    "\n",
    "    if not args.is_eval_only:\n",
    "\n",
    "        create_safe_directory(exp_dir)\n",
    "\n",
    "        # PREPARES DATA\n",
    "        train_loader = get_dataloaders(args.dataset,\n",
    "                                       batch_size=args.batch_size,\n",
    "                                       )\n",
    "\n",
    "        # PREPARES MODEL\n",
    "        args.img_size = get_img_size(args.dataset)  # stores for metadata\n",
    "        model = init_specific_model(args.model_type, args.img_size, args.latent_dim)\n",
    "        print('Num parameters in model: {}'.format(get_n_param(model)))\n",
    "\n",
    "        # TRAINS\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "        model = model.to(device)  # make sure trainer and viz on same device\n",
    "        # gif_visualizer = GifTraversalsTraining(model, args.dataset, exp_dir)\n",
    "        loss_f = get_loss_f(args.loss,\n",
    "                            n_data=len(train_loader.dataset),\n",
    "                            device=device,\n",
    "                            **vars(args))\n",
    "        trainer = Trainer(model, optimizer, loss_f,\n",
    "                          device=device,\n",
    "                          save_dir=exp_dir,\n",
    "                          is_progress_bar=not args.no_progress_bar #,\n",
    "                          #gif_visualizer=gif_visualizer\n",
    "                          )\n",
    "        trainer(train_loader,\n",
    "                epochs=args.epochs,\n",
    "                checkpoint_every=args.checkpoint_every,)\n",
    "\n",
    "        # SAVE MODEL AND EXPERIMENT INFORMATION\n",
    "        save_model(trainer.model, exp_dir, metadata=vars(args))\n",
    "\n",
    "    if args.is_metrics or not args.no_test:\n",
    "        model = load_model(exp_dir, is_gpu=not args.no_cuda)\n",
    "        metadata = load_metadata(exp_dir)\n",
    "        # TO-DO: currently uses train datatset\n",
    "        test_loader = get_dataloaders(metadata[\"dataset\"],\n",
    "                                      batch_size=args.eval_batchsize,\n",
    "                                      shuffle=False)\n",
    "        \n",
    "        loss_f = get_loss_f(args.loss,\n",
    "                            n_data=len(test_loader.dataset),\n",
    "                            device=device,\n",
    "                            **vars(args))\n",
    "        \n",
    "        evaluator = Evaluator(model, loss_f,\n",
    "                              device=device,\n",
    "                              save_dir=exp_dir,\n",
    "                              is_progress_bar=not args.no_progress_bar)\n",
    "\n",
    "        evaluator(test_loader, is_metrics=args.is_metrics, is_losses=not args.no_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters in model: 502005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "args_to_parse = 'juliana-tests-v9 -d chairs -l betaH --betaH-B 3 --lr 0.001 -b 64 -e 1'.split()\n",
    "\n",
    "args = parse_arguments(args_to_parse)\n",
    "main(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from utils.helpers import FormatterNoDuplicate, check_bounds, set_seed\n",
    "from utils.visualize import Visualizer\n",
    "from utils.viz_helpers import get_samples\n",
    "\n",
    "\n",
    "PLOT_TYPES = ['generate-samples', 'data-samples', 'reconstruct', \"traversals\",\n",
    "              'reconstruct-traverse', \"gif-traversals\", \"all\"]\n",
    "\n",
    "\n",
    "def parse_arguments_viz(args_to_parse):\n",
    "    \"\"\"Parse the command line arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args_to_parse: list of str\n",
    "        Arguments to parse (splitted on whitespaces).\n",
    "    \"\"\"\n",
    "    description = \"CLI for plotting using pretrained models of `disvae`\"\n",
    "    parser = argparse.ArgumentParser(description=description,\n",
    "                                     formatter_class=FormatterNoDuplicate)\n",
    "\n",
    "    parser.add_argument('name', type=str,\n",
    "                        help=\"Name of the model for storing and loading purposes.\")\n",
    "    parser.add_argument(\"plots\", type=str, nargs='+', choices=PLOT_TYPES,\n",
    "                        help=\"List of all plots to generate. `generate-samples`: random decoded samples. `data-samples` samples from the dataset. `reconstruct` first rnows//2 will be the original and rest will be the corresponding reconstructions. `traversals` traverses the most important rnows dimensions with ncols different samples from the prior or posterior. `reconstruct-traverse` first row for original, second are reconstructions, rest are traversals. `gif-traversals` grid of gifs where rows are latent dimensions, columns are examples, each gif shows posterior traversals. `all` runs every plot.\")\n",
    "    parser.add_argument('-s', '--seed', type=int, default=None,\n",
    "                        help='Random seed. Can be `None` for stochastic behavior.')\n",
    "    parser.add_argument('-r', '--n-rows', type=int, default=6,\n",
    "                        help='The number of rows to visualize (if applicable).')\n",
    "    parser.add_argument('-c', '--n-cols', type=int, default=7,\n",
    "                        help='The number of columns to visualize (if applicable).')\n",
    "    parser.add_argument('-t', '--max-traversal', default=2,\n",
    "                        type=lambda v: check_bounds(v, lb=0, is_inclusive=False,\n",
    "                                                    type=float, name=\"max-traversal\"),\n",
    "                        help='The maximum displacement induced by a latent traversal. Symmetrical traversals are assumed. If `m>=0.5` then uses absolute value traversal, if `m<0.5` uses a percentage of the distribution (quantile). E.g. for the prior the distribution is a standard normal so `m=0.45` corresponds to an absolute value of `1.645` because `2m=90%%` of a standard normal is between `-1.645` and `1.645`. Note in the case of the posterior, the distribution is not standard normal anymore.')\n",
    "    parser.add_argument('-i', '--idcs', type=int, nargs='+', default=[],\n",
    "                        help='List of indices to of images to put at the begining of the samples.')\n",
    "    parser.add_argument('-u', '--upsample-factor', default=1,\n",
    "                        type=lambda v: check_bounds(v, lb=1, is_inclusive=True,\n",
    "                                                    type=int, name=\"upsample-factor\"),\n",
    "                        help='The scale factor with which to upsample the image (if applicable).')\n",
    "    parser.add_argument('--is-show-loss', action='store_true',\n",
    "                        help='Displays the loss on the figures (if applicable).')\n",
    "    parser.add_argument('--is-posterior', action='store_true',\n",
    "                        help='Traverses the posterior instead of the prior.')\n",
    "    args = parser.parse_args(args_to_parse)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def main_viz(args):\n",
    "    \"\"\"Main function for plotting fro pretrained models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args: argparse.Namespace\n",
    "        Arguments\n",
    "    \"\"\"\n",
    "    set_seed(args.seed)\n",
    "    experiment_name = args.name\n",
    "    model_dir = os.path.join(RES_DIR, experiment_name)\n",
    "    meta_data = load_metadata(model_dir)\n",
    "    model = load_model(model_dir)\n",
    "    model.eval()  # don't sample from latent: use mean\n",
    "    dataset = meta_data['dataset']\n",
    "    viz = Visualizer(model=model,\n",
    "                     model_dir=model_dir,\n",
    "                     dataset=dataset,\n",
    "                     max_traversal=args.max_traversal,\n",
    "                     loss_of_interest='kl_loss_',\n",
    "                     upsample_factor=args.upsample_factor)\n",
    "    size = (args.n_rows, args.n_cols)\n",
    "    # same samples for all plots: sample max then take first `x`data  for all plots\n",
    "    num_samples = args.n_cols * args.n_rows\n",
    "    samples = get_samples(dataset, num_samples, idcs=args.idcs)\n",
    "\n",
    "    if \"all\" in args.plots:\n",
    "        args.plots = [p for p in PLOT_TYPES if p != \"all\"]\n",
    "\n",
    "    for plot_type in args.plots:\n",
    "        if plot_type == 'generate-samples':\n",
    "            viz.generate_samples(size=size)\n",
    "        elif plot_type == 'data-samples':\n",
    "            viz.data_samples(samples, size=size)\n",
    "        elif plot_type == \"reconstruct\":\n",
    "            viz.reconstruct(samples, size=size)\n",
    "        elif plot_type == 'traversals':\n",
    "            viz.traversals(data=samples[0:1, ...] if args.is_posterior else None,\n",
    "                           n_per_latent=args.n_cols,\n",
    "                           n_latents=args.n_rows,\n",
    "                           is_reorder_latents=True)\n",
    "        elif plot_type == \"reconstruct-traverse\":\n",
    "            viz.reconstruct_traverse(samples,\n",
    "                                     is_posterior=args.is_posterior,\n",
    "                                     n_latents=args.n_rows,\n",
    "                                     n_per_latent=args.n_cols,\n",
    "                                     is_show_text=args.is_show_loss)\n",
    "        elif plot_type == \"gif-traversals\":\n",
    "            viz.gif_traversals(samples[:args.n_cols, ...], n_latents=args.n_rows)\n",
    "        else:\n",
    "            raise ValueError(\"Unkown plot_type={}\".format(plot_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['juliana-tests-v9', 'all', '--is-show-loss']\n",
      "Selected idcs: [57767, 15315, 981, 11880, 76313, 4577, 11001, 12900, 46482, 31039, 2284, 4038, 2076, 45362, 84976, 81694, 63471, 80716, 60758, 19471, 11965, 23998, 14996, 1916, 65940, 63799, 32745, 8462, 70921, 61066, 8987, 78835, 11656, 67507, 75891, 6024, 35335, 72932, 8267, 84493, 39639, 63050]\n"
     ]
    }
   ],
   "source": [
    "args_viz = 'juliana-tests-v9 all --is-show-loss'.split()\n",
    "print(args_viz)\n",
    "args = parse_arguments_viz(args_viz)\n",
    "main_viz(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
