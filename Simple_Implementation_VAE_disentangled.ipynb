{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:07<00:00, 1260805.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1296517.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Epoch [1/10], Batch [1/469], Loss: 70227.3906\n",
      "Epoch [1/10], Batch [101/469], Loss: -2365557.7500\n",
      "Epoch [1/10], Batch [201/469], Loss: -2497822.7500\n",
      "Epoch [1/10], Batch [301/469], Loss: -2744792.7500\n",
      "Epoch [1/10], Batch [401/469], Loss: -2805517.7500\n",
      "Epoch [1/10], Average Loss: -2540833.1669\n",
      "Epoch [2/10], Batch [1/469], Loss: -2872397.0000\n",
      "Epoch [2/10], Batch [101/469], Loss: -3277094.0000\n",
      "Epoch [2/10], Batch [201/469], Loss: -3411370.2500\n",
      "Epoch [2/10], Batch [301/469], Loss: -3724626.7500\n",
      "Epoch [2/10], Batch [401/469], Loss: -4023608.0000\n",
      "Epoch [2/10], Average Loss: -3594711.4947\n",
      "Epoch [3/10], Batch [1/469], Loss: -3992253.2500\n",
      "Epoch [3/10], Batch [101/469], Loss: -4178103.2500\n",
      "Epoch [3/10], Batch [201/469], Loss: -4611663.0000\n",
      "Epoch [3/10], Batch [301/469], Loss: -4585065.0000\n",
      "Epoch [3/10], Batch [401/469], Loss: -4743047.5000\n",
      "Epoch [3/10], Average Loss: -4578719.1119\n",
      "Epoch [4/10], Batch [1/469], Loss: -5054668.5000\n",
      "Epoch [4/10], Batch [101/469], Loss: -5087439.0000\n",
      "Epoch [4/10], Batch [201/469], Loss: -5219766.5000\n",
      "Epoch [4/10], Batch [301/469], Loss: -5046000.5000\n",
      "Epoch [4/10], Batch [401/469], Loss: -5183530.5000\n",
      "Epoch [4/10], Average Loss: -5115208.2319\n",
      "Epoch [5/10], Batch [1/469], Loss: -5240879.0000\n",
      "Epoch [5/10], Batch [101/469], Loss: -5444941.0000\n",
      "Epoch [5/10], Batch [201/469], Loss: -5764567.5000\n",
      "Epoch [5/10], Batch [301/469], Loss: -5530628.0000\n",
      "Epoch [5/10], Batch [401/469], Loss: -5472916.5000\n",
      "Epoch [5/10], Average Loss: -5513009.4995\n",
      "Epoch [6/10], Batch [1/469], Loss: -5793721.5000\n",
      "Epoch [6/10], Batch [101/469], Loss: -5745516.0000\n",
      "Epoch [6/10], Batch [201/469], Loss: -5925068.0000\n",
      "Epoch [6/10], Batch [301/469], Loss: -5903295.0000\n",
      "Epoch [6/10], Batch [401/469], Loss: -6324905.0000\n",
      "Epoch [6/10], Average Loss: -5875626.7313\n",
      "Epoch [7/10], Batch [1/469], Loss: -6136196.0000\n",
      "Epoch [7/10], Batch [101/469], Loss: -6026100.5000\n",
      "Epoch [7/10], Batch [201/469], Loss: -6322404.0000\n",
      "Epoch [7/10], Batch [301/469], Loss: -6179010.5000\n",
      "Epoch [7/10], Batch [401/469], Loss: -6149199.0000\n",
      "Epoch [7/10], Average Loss: -6141622.8156\n",
      "Epoch [8/10], Batch [1/469], Loss: -6273959.5000\n",
      "Epoch [8/10], Batch [101/469], Loss: -6407158.0000\n",
      "Epoch [8/10], Batch [201/469], Loss: -6533234.0000\n",
      "Epoch [8/10], Batch [301/469], Loss: -6462954.5000\n",
      "Epoch [8/10], Batch [401/469], Loss: -6334863.5000\n",
      "Epoch [8/10], Average Loss: -6369226.7356\n",
      "Epoch [9/10], Batch [1/469], Loss: -6296065.5000\n",
      "Epoch [9/10], Batch [101/469], Loss: -6429112.5000\n",
      "Epoch [9/10], Batch [201/469], Loss: -6596885.0000\n",
      "Epoch [9/10], Batch [301/469], Loss: -6565050.0000\n",
      "Epoch [9/10], Batch [401/469], Loss: -6765749.5000\n",
      "Epoch [9/10], Average Loss: -6554674.0064\n",
      "Epoch [10/10], Batch [1/469], Loss: -6568480.0000\n",
      "Epoch [10/10], Batch [101/469], Loss: -6412430.5000\n",
      "Epoch [10/10], Batch [201/469], Loss: -6501240.5000\n",
      "Epoch [10/10], Batch [301/469], Loss: -6757051.0000\n",
      "Epoch [10/10], Batch [401/469], Loss: -6925073.0000\n",
      "Epoch [10/10], Average Loss: -6695528.0896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        mean = self.fc_mean(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mean + epsilon * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        hidden = F.relu(self.fc2(z))\n",
    "        reconstructed = torch.sigmoid(self.fc3(hidden))\n",
    "        return reconstructed\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mean, logvar\n",
    "\n",
    "def loss_function(reconstructed, x, mean, logvar):\n",
    "    # Reconstruction loss (binary cross-entropy)\n",
    "    reconstruction_loss = F.binary_cross_entropy(reconstructed, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss\n",
    "\n",
    "# Preprocess MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Parameters\n",
    "input_dim = 784  # Dimension of input (28x28 pixels images in MNIST)\n",
    "hidden_dim = 256  # Hidden dimension of encoder and decoder\n",
    "latent_dim = 10  # Dimension of latent representation\n",
    "\n",
    "# Create the VAE model\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "def train(model, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            # Prepare the data\n",
    "            data = data.view(-1, input_dim)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed, mean, logvar = model(data)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(reconstructed, data, mean, logvar)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print training information\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, batch_idx+1, len(train_loader), loss.item()))\n",
    "        \n",
    "        print('Epoch [{}/{}], Average Loss: {:.4f}'\n",
    "              .format(epoch+1, num_epochs, train_loss/len(train_loader)))\n",
    "\n",
    "# Call the training function\n",
    "num_epochs = 10\n",
    "train(model, optimizer, num_epochs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate new samples\n",
    "def generate_samples(model, num_samples):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim)\n",
    "        generated_samples = model.decode(z)\n",
    "    return generated_samples\n",
    "\n",
    "# Generate and display new samples\n",
    "num_samples = 10\n",
    "generated_samples = generate_samples(model, num_samples)\n",
    "\n",
    "# Reshape and denormalize the generated samples\n",
    "generated_samples = generated_samples.view(-1, 28, 28)\n",
    "generated_samples = (generated_samples * 0.5) + 0.5  # Denormalize\n",
    "\n",
    "# Plot the generated samples\n",
    "fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "for i in range(num_samples):\n",
    "    axes[i].imshow(generated_samples[i].cpu().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generated dimages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
