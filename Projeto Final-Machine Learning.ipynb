{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "magnetic-singing",
   "metadata": {},
   "source": [
    "## Projeto Final de Machine Learning (2023.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-advertiser",
   "metadata": {},
   "source": [
    "**Integrantes:**  Ademir Tomaz, Carlos Souza, Juliana Carvalho, Raphael Levy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-bumper",
   "metadata": {},
   "source": [
    "### Introdução:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-provision",
   "metadata": {},
   "source": [
    "### Fundamentação Teórica:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-latin",
   "metadata": {},
   "source": [
    "### Variantes de Autoencoders:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-needle",
   "metadata": {},
   "source": [
    "### Aplicações de Autoencoders e Disentangled Representation:\n",
    "\n",
    "De forma generalizada, podemos definir disentangled representation, ou representação desemaranhada, como uma técnica de aprendizado não-supervisionado que separa cada feature em variáveis restritas e codifica elas em dimensões distintas, de forma a capturar informações sensíveis e úteis e entendendo relações causais entre as variáveis. Por exemplo, na imagem abaixo é possível visualizar como podemos alterar dimensões específicas e entender como variações específicas modificam a imagem original:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-queen",
   "metadata": {},
   "source": [
    "![convert notebook to web app](https://ars.els-cdn.com/content/image/1-s2.0-S1361841522001633-gr1.jpg)\n",
    "\n",
    "Imagem retirada do artigo \"Learning disentangled representations in the imaging domain\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-clothing",
   "metadata": {},
   "source": [
    "Para definir \"desemaranhamento\" de uma maneira mais formal, precisamos ter em mente representações latentes, que são os dados comprimidos após serem passados pelo encoder, ou seja, transformando uma informação complexa em uma versão \"mais simples\", facilitando seu processamento e análise.\n",
    "\n",
    "Agora, tomando as definições necessárias:\n",
    "\n",
    "$\\mathcal{X}$: variáveis observadas\n",
    "\n",
    "$\\mathcal{Z}$: representações latentes\n",
    "\n",
    "$\\mathcal{G}$: fatores geradores\n",
    "\n",
    "$\\mathcal{Y}$: domínio de saídas do modelo\n",
    "\n",
    "O aprendizado do modelo é dado por uma função $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$, sendo que $f$ pode ser separado em dois compontentes:\n",
    "\n",
    "$$\n",
    "f : E_{\\phi} \\circ D_{\\phi},\n",
    "$$\n",
    "\n",
    "onde $E_{\\phi} : \\mathcal{X} \\rightarrow \\mathcal{Z}$ é o encoder, e $D_{\\phi} : \\mathcal{Z} \\rightarrow \\mathcal{Y}$ é o decoder, mapeando os dados para uma representação latente intermediária e a representação para os outputs, respectivamente.\n",
    "\n",
    "Assim, o objetivo do aprendizado do modelo é aprender uma boa representação, que terá equivariâncias, invariâncias e simetrias para diferentes alterações. Vamos definir essas operações:\n",
    "\n",
    "1. Simetria: uma simetria $\\Omega$ é uma transformação que mantém aspectos dos dados de entrada, como a categoria de um objeto por exemplo, que não se altera quando deslocamos o objeto. \n",
    "\n",
    "2. Equivariância: um mapeamento $E_{\\phi}$ é equivariante com respeito a $\\Omega$ se existe uma transformação $\\omega \\in \\Omega$ de um input $X \\in \\mathcal{X}$ que afeta o output $Z \\in \\mathcal{Z}$ da mesma maneira, ou seja, existe um mapeamento $M_{\\omega} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$ tal que $E_{\\phi}(M_{\\omega} \\circ X) = M_{\\omega} \\circ E_{\\phi}(X) \\ \\forall \\ \\omega \\in \\Omega$.\n",
    "\n",
    "3. Invariância: é um caso especial da equivariância, onde $M$ é a identidade, e $E_{\\phi}(M_{\\omega} \\circ X) = E_{\\phi}(X) \\ \\forall \\ \\omega \\in \\Omega$.\n",
    "\n",
    "Em resumo, usamos equivariância caso desejamos que a saída passe pela mesma transformação aplicada ao dado de entrada, enquanto que para a invariância, desejamos que a saída seja a mesma independente de qualquer transformação aplicada aos dados de entrada.\n",
    "\n",
    "Partindo agora para os fatores geradores, eles são as variáveis subjacentes que caracterizam a variação dos dados de $\\mathcal{X}$, que são observadas ou que esperam-se ser observadas. Sendo assim, representações deveriam possibilitar a decomposição, ou desemaranhamento, dos dados de entrada em fatores distintos, ou seja, cada fator corresponderia a uma única variável de interesse do processo que gerou os dados. Outro ponto necessário para uma definição mais formal dessas representações são as alterações de domínio, que ocorrem quando os dados de treino, validação e teste são extraídos de uma distribuição de probabilidade que é distinta da distribuição usada com os modelos de predição.\n",
    "\n",
    "Sendo assim, definimos uma representação desemaranhada:\n",
    "\n",
    "Seja $W$ o espaço de estados sujeito às transformações, por exemplo um espaço 2D com movimentações para cima, baixo, direita e esquerda; ou um espaço 3D de rotações de $90^{\\circ}$ não comutativas,e $\\Omega$ um conjunto de transformações. Supondo a existência de um processo generativo $b : W \\rightarrow O$ que leva esse espaço para as observações, e um processo de inferência $h : O \\rightarrow Z$ levando as observações às representações dos agentes. Em geral, assumimos $W$ um conjunto e $Z$ um espaço vetorial. Sendo assim, consideramos a composição $f : W \\rightarrow Z$, que consegue induzir $\\Omega$ na representação latente $Z \\in \\mathcal{Z}$ de forma equivariante. \n",
    "\n",
    "A representação $Z$ é definida como \"desemaranhada\" caso exista uma decomposição $Z = Z_1 \\times \\ldots \\times Z_n$ tal que uma transformação $\\omega$ aplicada em $Z_i$ resulta em uma transformação equivalente no domínio $\\mathcal{X}$, deixando inalterados quaisquer outros aspectos $Z_j, j \\neq i$. Essa definição satisfaz as propriedades desejadas para uma representação desemaranhada:\n",
    "\n",
    "1. Modularidade: essa propriedade mede se cada dimensão latente codifica não mais do que um único fator gerador. Cada código latente é associado a um único fator gerador e não é afetado por mudanças nos demais, sendo implicitamente necessário que cada variável latente seja separável. \n",
    "\n",
    "2. Compacidade (\"Compactness\"): mede se cada fator gerador de dados é codificado por uma única dimensão latente. Se a dimensionalidade do espaço latente for grande demais, informação ruidosa ou redundante pode ser codificada na representação latente. Essa definição não é bem acordada entre autores, visto que várias das abordagens mais recentes requerem que essa propriedade seja válida, enquanto que outras abordagens aceitam que fatores geradores individuais sejam codificados por múltiplas dimensões latentes.\n",
    "\n",
    "\n",
    "3. Explicitude: mede se os valores de todos os fatores generativos podem ser decodificados da representação usando uma transformação linear. É a propriedade mais forte das três, dado que engloba dois pontos de extrema importância: que uma representação desemaranhada capture informação sobre todos os fatores generativos (completude), e que essa informação seja linearmente decodificável (informatividade). \n",
    "\n",
    "A modularidade é o requerimento mais essencial para um modelo de aprendizado de representação desemaranhada, por garantir que cada feature desemaranhada corresponda a um fator generativo que as features desemaranhadas distintas sejam independentes umas das outras. A compacidade requer que o modelo mapeie os dados originais de alta dimensão a features de baixa dimensão, atuando como um compressor dos dados. Separando a explicitude em seus pontos integrantes, a completude requer que a representação desemaranhada expresse informação sobre todas as variáveis dos dados observados, enquanto que a informatividade se refere ao fato de que features desemaranhadas podem recuperar os valores dos fatores generativos através do modelo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-ethnic",
   "metadata": {},
   "source": [
    "### Metodologia:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-graphics",
   "metadata": {},
   "source": [
    "### Discussão e Resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-constitution",
   "metadata": {},
   "source": [
    "### Conclusão:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-terrace",
   "metadata": {},
   "source": [
    "### Referências:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-pencil",
   "metadata": {},
   "source": [
    "1. \"Autoencoder Image Interpolation by Shaping the Latent Space\", Oring et al. https://arxiv.org/abs/2008.01487\n",
    "\n",
    "2. \"Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer\", Berthelot et al. https://arxiv.org/abs/1807.07543\n",
    "\n",
    "3. \"Learning disentangled representations in the imaging domain\", Liu et al. https://www.sciencedirect.com/science/article/pii/S1361841522001633?via%3Dihub\n",
    "\n",
    "4. \"Invariant-equivariant representation learning for multi-class data\", Ilya Feige. https://arxiv.org/abs/1902.03251\n",
    "\n",
    "5. \"Deep Learning – Equivariance and Invariance\", Bernhard Kainz. https://www.doc.ic.ac.uk/~bkainz/teaching/DL/notes/equivariance.pdf\n",
    "\n",
    "6. \"Domain shift\", Marco Taboga. https://www.statlect.com/machine-learning/domain-shift \n",
    "\n",
    "7. \"A Review of Disentangled Representation Learning for Remote Sensing Data\", Wang et al. https://www.sciopen.com/article/10.26599/AIR.2022.9150012\n",
    "\n",
    "8. \"Towards a Definition of Disentangled Representations\", Higgins et al. https://arxiv.org/pdf/1812.02230.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-addition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
